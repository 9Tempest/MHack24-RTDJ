{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Human Action Recognition with OpenVINO™\n",
    "\n",
    "This notebook demonstrates live human action recognition with OpenVINO, using the [Action Recognition Models](https://docs.openvino.ai/2020.2/usergroup13.html) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo), specifically an [Encoder](https://docs.openvino.ai/2020.2/_models_intel_action_recognition_0001_encoder_description_action_recognition_0001_encoder.html) and a [Decoder](https://docs.openvino.ai/2020.2/_models_intel_action_recognition_0001_decoder_description_action_recognition_0001_decoder.html). Both models create a sequence to sequence (`\"seq2seq\"`) \\[1\\] system to identify the human activities for [Kinetics-400 dataset](https://deepmind.com/research/open-source/kinetics). The models use the Video Transformer approach with ResNet34 encoder \\[2\\]. The notebook shows how to create the following pipeline:\n",
    "\n",
    "<img align='center' src=\"https://user-images.githubusercontent.com/10940214/148401661-477aebcd-f2d0-4771-b107-4b37f94d0b1e.jpeg\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "Final part of this notebook shows live inference results from a webcam. Additionally, you can also upload a video file.\n",
    "\n",
    "**NOTE**: To use a webcam, you must run this Jupyter notebook on a computer with a webcam. If you run on a server, the webcam will not work. However, you can still do inference on a video in the final step.\n",
    "\n",
    "---\n",
    "\n",
    "\\[1\\] seq2seq: Deep learning models that take a sequence of items to the input and output. In this case, input: video frames, output: actions sequence. This `\"seq2seq\"` is composed of an encoder and a decoder. The encoder captures `\"context\"` of the inputs to be analyzed by the decoder, and finally gets the human action and confidence.\n",
    "\n",
    "\\[2\\] [Video Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) and [ResNet34](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet34.html).\n",
    "\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Imports](#Imports)\n",
    "- [The models](#The-models)\n",
    "    - [Download the models](#Download-the-models)\n",
    "    - [Load your labels](#Load-your-labels)\n",
    "    - [Load the models](#Load-the-models)\n",
    "        - [Model Initialization function](#Model-Initialization-function)\n",
    "        - [Initialization for Encoder and Decoder](#Initialization-for-Encoder-and-Decoder)\n",
    "    - [Helper functions](#Helper-functions)\n",
    "    - [AI Functions](#AI-Functions)\n",
    "    - [Main Processing Function](#Main-Processing-Function)\n",
    "    - [Run Action Recognition](#Run-Action-Recognition)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/action-recognition-webcam/action-recognition-webcam.ipynb\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"openvino>=2024.0.0\" \"opencv-python\" \"tqdm\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, List\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "import openvino as ov\n",
    "from openvino.runtime.ie_api import CompiledModel\n",
    "\n",
    "# Fetch `notebook_utils` module\n",
    "import requests\n",
    "\n",
    "# r = requests.get(\n",
    "#     url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\",\n",
    "# )\n",
    "# open(\"notebook_utils.py\", \"w\").write(r.text)\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "### Download the models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Use the `download_ir_model`, a function from the `notebook_utils` file. It automatically creates a directory structure and downloads the selected model.\n",
    "\n",
    "In this case you can use `\"action-recognition-0001\"` as a model name, and the system automatically downloads the two models `\"action-recognition-0001-encoder\"` and `\"action-recognition-0001-decoder\"`\n",
    "\n",
    "> **NOTE**: If you want to download another model, such as `\"driver-action-recognition-adas-0002\"` (`\"driver-action-recognition-adas-0002-encoder\"` + `\"driver-action-recognition-adas-0002-decoder\"`), replace the name of the model in the code below. Using a model outside the list can require different pre- and post-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A directory where the model will be downloaded.\n",
    "base_model_dir = \"model\"\n",
    "# The name of the model from Open Model Zoo.\n",
    "model_name = \"action-recognition-0001\"\n",
    "# Selected precision (FP32, FP16, FP16-INT8).\n",
    "precision = \"FP16\"\n",
    "model_path_decoder = f\"model/intel/{model_name}/{model_name}-decoder/{precision}/{model_name}-decoder.xml\"\n",
    "model_path_encoder = f\"model/intel/{model_name}/{model_name}-encoder/{precision}/{model_name}-encoder.xml\"\n",
    "encoder_url = f\"https://storage.openvinotoolkit.org/repositories/open_model_zoo/temp/{model_name}/{model_name}-encoder/{precision}/{model_name}-encoder.xml\"\n",
    "decoder_url = f\"https://storage.openvinotoolkit.org/repositories/open_model_zoo/temp/{model_name}/{model_name}-decoder/{precision}/{model_name}-decoder.xml\"\n",
    "\n",
    "if not os.path.exists(model_path_decoder):\n",
    "    utils.download_ir_model(decoder_url, Path(model_path_decoder).parent)\n",
    "if not os.path.exists(model_path_encoder):\n",
    "    utils.download_ir_model(encoder_url, Path(model_path_encoder).parent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your labels\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "This tutorial uses [Kinetics-400 dataset](https://deepmind.com/research/open-source/kinetics), and also provides the text file embedded into this notebook. \n",
    "\n",
    "> **NOTE**: If you want to run `\"driver-action-recognition-adas-0002\"` model, replace the `kinetics.txt` file to `driver_actions.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data\\kinetics.txt' already exists.\n",
      "['abseiling', 'air drumming', 'answering questions', 'applauding', 'applying cream', 'archery', 'arm wrestling', 'arranging flowers', 'assembling computer'] (400,)\n"
     ]
    }
   ],
   "source": [
    "# Download the text from the openvino_notebooks storage\n",
    "vocab_file_path = utils.download_file(\n",
    "    \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/text/kinetics.txt\",\n",
    "    directory=\"data\",\n",
    ")\n",
    "\n",
    "with vocab_file_path.open(mode=\"r\") as f:\n",
    "    labels = [line.strip() for line in f]\n",
    "\n",
    "print(labels[0:9], np.shape(labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load the models\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Load the two models for this particular architecture, Encoder and Decoder. Downloaded models are located in a fixed structure, indicating a vendor, the name of the model, and a precision.\n",
    "\n",
    " 1. Initialize OpenVINO Runtime.\n",
    " 2. Read the network from `*.bin` and `*.xml` files (weights and architecture).\n",
    " 3. Compile the model for specified device.\n",
    " 4. Get input and output names of nodes.\n",
    "\n",
    "Only a few lines of code are required to run the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce1478f6de446fe97105f5e75711cfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = utils.device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization function\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenVINO Runtime.\n",
    "core = ov.Core()\n",
    "\n",
    "\n",
    "def model_init(model_path: str, device: str) -> Tuple:\n",
    "    \"\"\"\n",
    "    Read the network and weights from a file, load the\n",
    "    model on CPU and get input and output names of nodes\n",
    "\n",
    "    :param:\n",
    "            model: model architecture path *.xml\n",
    "            device: inference device\n",
    "    :retuns:\n",
    "            compiled_model: Compiled model\n",
    "            input_key: Input node for model\n",
    "            output_key: Output node for model\n",
    "    \"\"\"\n",
    "\n",
    "    # Read the network and corresponding weights from a file.\n",
    "    model = core.read_model(model=model_path)\n",
    "    # Compile the model for specified device.\n",
    "    compiled_model = core.compile_model(model=model, device_name=device)\n",
    "    # Get input and output names of nodes.\n",
    "    input_keys = compiled_model.input(0)\n",
    "    output_keys = compiled_model.output(0)\n",
    "    return input_keys, output_keys, compiled_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization for Encoder and Decoder\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder initialization\n",
    "input_key_en, output_keys_en, compiled_model_en = model_init(model_path_encoder, device.value)\n",
    "# Decoder initialization\n",
    "input_key_de, output_keys_de, compiled_model_de = model_init(model_path_decoder, device.value)\n",
    "\n",
    "# Get input size - Encoder.\n",
    "height_en, width_en = list(input_key_en.shape)[2:]\n",
    "# Get input size - Decoder.\n",
    "frames2decode = list(input_key_de.shape)[0:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 16, 512]\n"
     ]
    }
   ],
   "source": [
    "print(list(input_key_de.shape)[0:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Use the following helper functions for preprocessing and postprocessing frames:\n",
    "\n",
    "1. Preprocess the input image before running the Encoder model. (`center_crop` and `adaptative_resize`)\n",
    "2. Decode top-3 probabilities into label names. (`decode_output`)\n",
    "3. Draw the Region of Interest (ROI) over the video. (`rec_frame_display`)\n",
    "4. Prepare the frame for displaying label names over the video. (`display_text_fnc`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(frame: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Center crop squared the original frame to standardize the input image to the encoder model\n",
    "\n",
    "    :param frame: input frame\n",
    "    :returns: center-crop-squared frame\n",
    "    \"\"\"\n",
    "    img_h, img_w, _ = frame.shape\n",
    "    min_dim = min(img_h, img_w)\n",
    "    start_x = int((img_w - min_dim) / 2.0)\n",
    "    start_y = int((img_h - min_dim) / 2.0)\n",
    "    roi = [start_y, (start_y + min_dim), start_x, (start_x + min_dim)]\n",
    "    return frame[start_y : (start_y + min_dim), start_x : (start_x + min_dim), ...], roi\n",
    "\n",
    "\n",
    "def adaptive_resize(frame: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "     The frame going to be resized to have a height of size or a width of size\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param size: input size to encoder model\n",
    "    :returns: resized frame, np.array type\n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "    scale = size / min(h, w)\n",
    "    w_scaled, h_scaled = int(w * scale), int(h * scale)\n",
    "    if w_scaled == w and h_scaled == h:\n",
    "        return frame\n",
    "    return cv2.resize(frame, (w_scaled, h_scaled))\n",
    "\n",
    "\n",
    "def decode_output(probs: np.ndarray, labels: np.ndarray, top_k: int = 3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Decodes top probabilities into corresponding label names\n",
    "\n",
    "    :param probs: confidence vector for 400 actions\n",
    "    :param labels: list of actions\n",
    "    :param top_k: The k most probable positions in the list of labels\n",
    "    :returns: decoded_labels: The k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence for the k most probable actions\n",
    "    \"\"\"\n",
    "    top_ind = np.argsort(-1 * probs)[:top_k]\n",
    "    out_label = np.array(labels)[top_ind.astype(int)]\n",
    "    decoded_labels = [out_label[0][0], out_label[0][1], out_label[0][2]]\n",
    "    top_probs = np.array(probs)[0][top_ind.astype(int)]\n",
    "    decoded_top_probs = [top_probs[0][0], top_probs[0][1], top_probs[0][2]]\n",
    "    return decoded_labels, decoded_top_probs\n",
    "\n",
    "\n",
    "def rec_frame_display(frame: np.ndarray, roi) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw a rec frame over actual frame\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param roi: Region of interest, image section processed by the Encoder\n",
    "    :returns: frame with drawed shape\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    cv2.line(frame, (roi[2] + 3, roi[0] + 3), (roi[2] + 3, roi[0] + 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[0] + 3), (roi[2] + 100, roi[0] + 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[1] - 3), (roi[3] - 3, roi[1] - 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[1] - 3), (roi[3] - 100, roi[1] - 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[0] + 3), (roi[3] - 3, roi[0] + 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[3] - 3, roi[0] + 3), (roi[3] - 100, roi[0] + 3), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[1] - 3), (roi[2] + 3, roi[1] - 100), (0, 200, 0), 2)\n",
    "    cv2.line(frame, (roi[2] + 3, roi[1] - 3), (roi[2] + 100, roi[1] - 3), (0, 200, 0), 2)\n",
    "    # Write ROI over actual frame\n",
    "    FONT_STYLE = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    org = (roi[2] + 3, roi[1] - 3)\n",
    "    org2 = (roi[2] + 2, roi[1] - 2)\n",
    "    FONT_SIZE = 0.5\n",
    "    FONT_COLOR = (0, 200, 0)\n",
    "    FONT_COLOR2 = (0, 0, 0)\n",
    "    cv2.putText(frame, \"ROI\", org2, FONT_STYLE, FONT_SIZE, FONT_COLOR2)\n",
    "    cv2.putText(frame, \"ROI\", org, FONT_STYLE, FONT_SIZE, FONT_COLOR)\n",
    "    return frame\n",
    "\n",
    "\n",
    "def display_text_fnc(frame: np.ndarray, display_text: str, index: int):\n",
    "    \"\"\"\n",
    "    Include a text on the analyzed frame\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param display_text: text to add on the frame\n",
    "    :param index: index line dor adding text\n",
    "\n",
    "    \"\"\"\n",
    "    # Configuration for displaying images with text.\n",
    "    FONT_COLOR = (255, 255, 255)\n",
    "    FONT_COLOR2 = (0, 0, 0)\n",
    "    FONT_STYLE = cv2.FONT_HERSHEY_DUPLEX\n",
    "    FONT_SIZE = 0.7\n",
    "    TEXT_VERTICAL_INTERVAL = 25\n",
    "    TEXT_LEFT_MARGIN = 15\n",
    "    # ROI over actual frame\n",
    "    (processed, roi) = center_crop(frame)\n",
    "    # Draw a ROI over actual frame.\n",
    "    frame = rec_frame_display(frame, roi)\n",
    "    # Put a text over actual frame.\n",
    "    text_loc = (TEXT_LEFT_MARGIN, TEXT_VERTICAL_INTERVAL * (index + 1))\n",
    "    text_loc2 = (TEXT_LEFT_MARGIN + 1, TEXT_VERTICAL_INTERVAL * (index + 1) + 1)\n",
    "    cv2.putText(frame, display_text, text_loc2, FONT_STYLE, FONT_SIZE, FONT_COLOR2)\n",
    "    cv2.putText(frame, display_text, text_loc, FONT_STYLE, FONT_SIZE, FONT_COLOR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Functions\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "<img align='center' src=\"https://user-images.githubusercontent.com/10940214/148401661-477aebcd-f2d0-4771-b107-4b37f94d0b1e.jpeg\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "Following the pipeline above, you will use the next functions to:\n",
    "\n",
    "1. Preprocess a frame before running the Encoder. (`preprocessing`)\n",
    "2. Encoder Inference per frame. (`encoder`)\n",
    "3. Decoder inference per set of frames. (`decoder`)\n",
    "4. Normalize the Decoder output to get confidence values per action recognition label. (`softmax`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(frame: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preparing frame before Encoder.\n",
    "    The image should be scaled to its shortest dimension at \"size\"\n",
    "    and cropped, centered, and squared so that both width and\n",
    "    height have lengths \"size\". The frame must be transposed from\n",
    "    Height-Width-Channels (HWC) to Channels-Height-Width (CHW).\n",
    "\n",
    "    :param frame: input frame\n",
    "    :param size: input size to encoder model\n",
    "    :returns: resized and cropped frame\n",
    "    \"\"\"\n",
    "    # Adaptative resize\n",
    "    preprocessed = adaptive_resize(frame, size)\n",
    "    # Center_crop\n",
    "    (preprocessed, roi) = center_crop(preprocessed)\n",
    "    # Transpose frame HWC -> CHW\n",
    "    preprocessed = preprocessed.transpose((2, 0, 1))[None,]  # HWC -> CHW\n",
    "    return preprocessed, roi\n",
    "\n",
    "\n",
    "def encoder(preprocessed: np.ndarray, compiled_model: CompiledModel) -> List:\n",
    "    \"\"\"\n",
    "    Encoder Inference per frame. This function calls the network previously\n",
    "    configured for the encoder model (compiled_model), extracts the data\n",
    "    from the output node, and appends it in an array to be used by the decoder.\n",
    "\n",
    "    :param: preprocessed: preprocessing frame\n",
    "    :param: compiled_model: Encoder model network\n",
    "    :returns: encoder_output: embedding layer that is appended with each arriving frame\n",
    "    \"\"\"\n",
    "    output_key_en = compiled_model.output(0)\n",
    "\n",
    "    # Get results on action-recognition-0001-encoder model\n",
    "    infer_result_encoder = compiled_model([preprocessed])[output_key_en]\n",
    "    return infer_result_encoder\n",
    "\n",
    "\n",
    "def decoder(encoder_output: List, compiled_model_de: CompiledModel) -> List:\n",
    "    \"\"\"\n",
    "    Decoder inference per set of frames. This function concatenates the embedding layer\n",
    "    froms the encoder output, transpose the array to match with the decoder input size.\n",
    "    Calls the network previously configured for the decoder model (compiled_model_de), extracts\n",
    "    the logits and normalize those to get confidence values along specified axis.\n",
    "    Decodes top probabilities into corresponding label names\n",
    "\n",
    "    :param: encoder_output: embedding layer for 16 frames\n",
    "    :param: compiled_model_de: Decoder model network\n",
    "    :returns: decoded_labels: The k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence for the k most probable actions\n",
    "    \"\"\"\n",
    "    # Concatenate sample_duration frames in just one array\n",
    "    decoder_input = np.concatenate(encoder_output, axis=0)\n",
    "    # Organize input shape vector to the Decoder (shape: [1x16x512]]\n",
    "    decoder_input = decoder_input.transpose((2, 0, 1, 3))\n",
    "    decoder_input = np.squeeze(decoder_input, axis=3)\n",
    "    output_key_de = compiled_model_de.output(0)\n",
    "    # Get results on action-recognition-0001-decoder model\n",
    "    result_de = compiled_model_de([decoder_input])[output_key_de]\n",
    "    # Normalize logits to get confidence values along specified axis\n",
    "    probs = softmax(result_de - np.max(result_de))\n",
    "    # Decodes top probabilities into corresponding label names\n",
    "    decoded_labels, decoded_top_probs = decode_output(probs, labels, top_k=3)\n",
    "    return decoded_labels, decoded_top_probs\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes logits to get confidence values along specified axis\n",
    "    x: np.array, axis=None\n",
    "    \"\"\"\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Main Processing Function\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Running action recognition function will run in different operations, either a webcam or a video file. See the list of procedures below:\n",
    "\n",
    "1. Create a video player to play with target fps (`utils.VideoPlayer`).\n",
    "2. Prepare a set of frames to be encoded-decoded.\n",
    "3. Run AI functions\n",
    "4. Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_action_recognition(\n",
    "    source: str = \"0\",\n",
    "    flip: bool = True,\n",
    "    use_popup: bool = False,\n",
    "    compiled_model_en: CompiledModel = compiled_model_en,\n",
    "    compiled_model_de: CompiledModel = compiled_model_de,\n",
    "    skip_first_frames: int = 0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use the \"source\" webcam or video file to run the complete pipeline for action-recognition problem\n",
    "    1. Create a video player to play with target fps\n",
    "    2. Prepare a set of frames to be encoded-decoded\n",
    "    3. Preprocess frame before Encoder\n",
    "    4. Encoder Inference per frame\n",
    "    5. Decoder inference per set of frames\n",
    "    6. Visualize the results\n",
    "\n",
    "    :param: source: webcam \"0\" or video path\n",
    "    :param: flip: to be used by VideoPlayer function for flipping capture image\n",
    "    :param: use_popup: False for showing encoded frames over this notebook, True for creating a popup window.\n",
    "    :param: skip_first_frames: Number of frames to skip at the beginning of the video.\n",
    "    :returns: display video over the notebook or in a popup window\n",
    "\n",
    "    \"\"\"\n",
    "    size = height_en  # Endoder input size - From Cell 5_9\n",
    "    sample_duration = frames2decode  # Decoder input size - From Cell 5_7\n",
    "    # Select frames per second of your source.\n",
    "    fps = 30\n",
    "    player = None\n",
    "    try:\n",
    "        # Create a video player.\n",
    "        player = utils.VideoPlayer(source, flip=flip, fps=fps, skip_first_frames=skip_first_frames)\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(title, cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        processing_time = 0\n",
    "        encoder_output = []\n",
    "        decoded_labels = [0, 0, 0]\n",
    "        decoded_top_probs = [0, 0, 0]\n",
    "        counter = 0\n",
    "        # Create a text template to show inference results over video.\n",
    "        text_inference_template = \"Infer Time:{Time:.1f}ms,{fps:.1f}FPS\"\n",
    "        text_template = \"{label},{conf:.2f}%\"\n",
    "\n",
    "        while True:\n",
    "            counter = counter + 1\n",
    "\n",
    "            # Read a frame from the video stream.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "\n",
    "            scale = 1280 / max(frame.shape)\n",
    "\n",
    "            # Adaptative resize for visualization.\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Select one frame every two for processing through the encoder.\n",
    "            # After 16 frames are processed, the decoder will find the action,\n",
    "            # and the label will be printed over the frames.\n",
    "\n",
    "            if counter % 2 == 0:\n",
    "                # Preprocess frame before Encoder.\n",
    "                (preprocessed, _) = preprocessing(frame, size)\n",
    "\n",
    "                # Measure processing time.\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Encoder Inference per frame\n",
    "                encoder_output.append(encoder(preprocessed, compiled_model_en))\n",
    "\n",
    "                # Decoder inference per set of frames\n",
    "                # Wait for sample duration to work with decoder model.\n",
    "                if len(encoder_output) == sample_duration:\n",
    "                    decoded_labels, decoded_top_probs = decoder(encoder_output, compiled_model_de)\n",
    "                    encoder_output = []\n",
    "\n",
    "                # Inference has finished. Display the results.\n",
    "                stop_time = time.time()\n",
    "\n",
    "                # Calculate processing time.\n",
    "                processing_times.append(stop_time - start_time)\n",
    "\n",
    "                # Use processing times from last 200 frames.\n",
    "                if len(processing_times) > 200:\n",
    "                    processing_times.popleft()\n",
    "\n",
    "                # Mean processing time [ms]\n",
    "                processing_time = np.mean(processing_times) * 1000\n",
    "                fps = 1000 / processing_time\n",
    "\n",
    "            # Visualize the results.\n",
    "            for i in range(0, 3):\n",
    "                display_text = text_template.format(\n",
    "                    label=decoded_labels[i],\n",
    "                    conf=decoded_top_probs[i] * 100,\n",
    "                )\n",
    "                display_text_fnc(frame, display_text, i)\n",
    "\n",
    "            display_text = text_inference_template.format(Time=processing_time, fps=fps)\n",
    "            display_text_fnc(frame, display_text, 3)\n",
    "\n",
    "            # Use this workaround if you experience flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(title, frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(\".jpg\", frame, params=[cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # Any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Action Recognition\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Find out how the model works in a video file. [Any format supported](https://docs.opencv.org/4.5.1/dd/d43/tutorial_py_video_display.html) by OpenCV will work. You can press the stop button anytime while the video file is running, and it will activate the webcam for the next step.\n",
    "\n",
    "> **NOTE**: Sometimes, the video can be cut off if there are corrupted frames. In that case, you can convert it. If you experience any problems with your video, use the [HandBrake](https://handbrake.fr/) and select the MPEG format.\n",
    "\n",
    "if you want to use a web camera as an input source for the demo, please change the value of `USE_WEBCAM` variable to True and specify `cam_id` (the default value is 0, which can be different in multi-camera systems)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": [],
    "test_replace": {
     "{\"skip_first_frames\": 600, \"flip\": False} if not USE_WEBCAM else {\"flip\": True}\n": "{\"skip_first_frames\": 1800, \"flip\": False} if not USE_WEBCAM else {\"flip\": True}\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "y = []\n",
    "x = []\n",
    "\n",
    "test = []\n",
    "\n",
    "import importlib\n",
    "import pose_estimation\n",
    "\n",
    "# Make some changes to mymodule.py\n",
    "#\n",
    "\n",
    "# Reload the module to apply changes\n",
    "importlib.reload(pose_estimation)\n",
    "\n",
    "frames2decode = 1\n",
    "#Process single entry from dataset\n",
    "def process_video(video_file, ground_truth):\n",
    "\n",
    "    \n",
    "    #TODO: Replace with dataloader\n",
    "    \n",
    "    source = video_file\n",
    "\n",
    "    skip_first_frames = 600\n",
    "    flip = False\n",
    "    \n",
    "    size = height_en  # Endoder input size - From Cell 5_9\n",
    "    sample_duration = frames2decode  # Decoder input size - From Cell 5_7\n",
    "    # Select frames per second of your source.\n",
    "    fps = 30\n",
    "    player = None\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        max_frames = 10\n",
    "        counter = 0\n",
    "        encoder_output = []\n",
    "        while(cap.isOpened()):\n",
    "            ret, frame = cap.read()\n",
    "            counter = counter + 1\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "    \n",
    "            scale = 1280 / max(frame.shape)\n",
    "    \n",
    "            # Adaptative resize for visualization.\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "                #cv2.imshow(frame)\n",
    "    \n",
    "            # Select one frame every two for processing through the encoder.\n",
    "            # After 16 frames are processed, the decoder will find the action,\n",
    "            # and the label will be printed over the frames.\n",
    "    \n",
    "            if counter % 200 == 0:\n",
    "                # Preprocess frame before Encoder.\n",
    "                (preprocessed, _) = preprocessing(frame, size)\n",
    "    \n",
    "                # Measure processing time.\n",
    "                start_time = time.time()\n",
    "    \n",
    "                # Encoder Inference per frame\n",
    "                encoder_output=encoder(preprocessed, compiled_model_en)\n",
    "                encoded_poses= pose_estimation.encoder_poses(frame,cv2.resize(frame, (456, 256), interpolation=cv2.INTER_AREA).transpose((2, 0, 1))[np.newaxis, ...])\n",
    "                \n",
    "                encoder_output = np.concatenate((encoder_output.flatten(),encoded_poses.flatten())).tolist()\n",
    "                \n",
    "                # Decoder inference per set of frames\n",
    "                # Wait for sample duration to work with decoder model.\n",
    "                #if len(encoder_output) == sample_duration:\n",
    "                x.append(encoder_output)\n",
    "                y.append(ground_truth)\n",
    "                encoder_output = []\n",
    "                max_frames -= 1;\n",
    "                if max_frames <= 0:\n",
    "                    break;\n",
    "\n",
    "                #fps = 1000 / processing_time\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        cap.release()\n",
    "    # Any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        cap.release()\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "{'track_id': '3CzWF2ZTazx5caTSLckEj5', 'track_name': 'ID - ID', 'danceability': 0.556, 'energy': 0.742, 'tempo': 116.958, 'valence': 0.284, 'video_file': '1.mp4'}\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n",
      "[<Dimension: 1>, <Dimension: 38>, <Dimension: 32>, <Dimension: 57>]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "video_file = \"https://archive.org/serve/ISSVideoResourceLifeOnStation720p/ISS%20Video%20Resource_LifeOnStation_720p.mp4\"\n",
    "\n",
    "train_dset = json.load(open('audio_features.json', 'r'))\n",
    "\n",
    "print(len(train_dset))\n",
    "print(train_dset[0])\n",
    "\n",
    "def get_features(thing):\n",
    "    return [thing[\"danceability\"], thing[\"energy\"],thing[\"tempo\"] / float(180),thing[\"valence\"]]\n",
    "\n",
    "# for i in train_dset.values():\n",
    "#     print(i)\n",
    "    \n",
    "for i in range(len(train_dset)-1):\n",
    "    process_video(\"videos/\"+train_dset[i]['video_file'], get_features(train_dset[i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.727, 0.926, 0.7500555555555555, 0.417], [0.727, 0.926, 0.7500555555555555, 0.417], [0.727, 0.926, 0.7500555555555555, 0.417], [0.727, 0.926, 0.7500555555555555, 0.417], [0.727, 0.926, 0.7500555555555555, 0.417], [0.727, 0.926, 0.7500555555555555, 0.417], [0.727, 0.926, 0.7500555555555555, 0.417], [0.727, 0.926, 0.7500555555555555, 0.417], [0.727, 0.926, 0.7500555555555555, 0.417], [0.727, 0.926, 0.7500555555555555, 0.417], [0.658, 0.662, 0.6834166666666667, 0.254], [0.658, 0.662, 0.6834166666666667, 0.254], [0.658, 0.662, 0.6834166666666667, 0.254], [0.658, 0.662, 0.6834166666666667, 0.254], [0.658, 0.662, 0.6834166666666667, 0.254], [0.658, 0.662, 0.6834166666666667, 0.254], [0.658, 0.662, 0.6834166666666667, 0.254], [0.658, 0.662, 0.6834166666666667, 0.254], [0.658, 0.662, 0.6834166666666667, 0.254], [0.658, 0.662, 0.6834166666666667, 0.254], [0.705, 0.672, 0.6888888888888889, 0.284], [0.705, 0.672, 0.6888888888888889, 0.284], [0.705, 0.672, 0.6888888888888889, 0.284], [0.705, 0.672, 0.6888888888888889, 0.284], [0.705, 0.672, 0.6888888888888889, 0.284], [0.705, 0.672, 0.6888888888888889, 0.284], [0.705, 0.672, 0.6888888888888889, 0.284], [0.705, 0.672, 0.6888888888888889, 0.284], [0.705, 0.672, 0.6888888888888889, 0.284], [0.705, 0.672, 0.6888888888888889, 0.284], [0.699, 0.884, 0.7115444444444444, 0.287], [0.699, 0.884, 0.7115444444444444, 0.287], [0.699, 0.884, 0.7115444444444444, 0.287], [0.699, 0.884, 0.7115444444444444, 0.287], [0.699, 0.884, 0.7115444444444444, 0.287], [0.699, 0.884, 0.7115444444444444, 0.287], [0.699, 0.884, 0.7115444444444444, 0.287], [0.699, 0.884, 0.7115444444444444, 0.287], [0.699, 0.884, 0.7115444444444444, 0.287], [0.699, 0.884, 0.7115444444444444, 0.287], [0.575, 0.922, 0.8168777777777778, 0.978], [0.575, 0.922, 0.8168777777777778, 0.978], [0.575, 0.922, 0.8168777777777778, 0.978], [0.575, 0.922, 0.8168777777777778, 0.978], [0.575, 0.922, 0.8168777777777778, 0.978], [0.575, 0.922, 0.8168777777777778, 0.978], [0.575, 0.922, 0.8168777777777778, 0.978], [0.575, 0.922, 0.8168777777777778, 0.978], [0.575, 0.922, 0.8168777777777778, 0.978], [0.575, 0.922, 0.8168777777777778, 0.978], [0.772, 0.973, 0.7778388888888889, 0.836], [0.772, 0.973, 0.7778388888888889, 0.836], [0.772, 0.973, 0.7778388888888889, 0.836], [0.772, 0.973, 0.7778388888888889, 0.836], [0.772, 0.973, 0.7778388888888889, 0.836], [0.772, 0.973, 0.7778388888888889, 0.836], [0.772, 0.973, 0.7778388888888889, 0.836], [0.772, 0.973, 0.7778388888888889, 0.836], [0.772, 0.973, 0.7778388888888889, 0.836], [0.772, 0.973, 0.7778388888888889, 0.836], [0.747, 0.821, 0.8057388888888888, 0.787], [0.747, 0.821, 0.8057388888888888, 0.787], [0.747, 0.821, 0.8057388888888888, 0.787], [0.747, 0.821, 0.8057388888888888, 0.787], [0.747, 0.821, 0.8057388888888888, 0.787], [0.747, 0.821, 0.8057388888888888, 0.787], [0.747, 0.821, 0.8057388888888888, 0.787], [0.747, 0.821, 0.8057388888888888, 0.787], [0.747, 0.821, 0.8057388888888888, 0.787], [0.747, 0.821, 0.8057388888888888, 0.787], [0.667, 0.94, 0.7944166666666667, 0.875], [0.667, 0.94, 0.7944166666666667, 0.875], [0.667, 0.94, 0.7944166666666667, 0.875], [0.667, 0.94, 0.7944166666666667, 0.875], [0.667, 0.94, 0.7944166666666667, 0.875], [0.667, 0.94, 0.7944166666666667, 0.875], [0.667, 0.94, 0.7944166666666667, 0.875], [0.667, 0.94, 0.7944166666666667, 0.875], [0.667, 0.94, 0.7944166666666667, 0.875], [0.667, 0.94, 0.7944166666666667, 0.875], [0.682, 0.944, 0.8277777777777777, 0.735], [0.682, 0.944, 0.8277777777777777, 0.735], [0.682, 0.944, 0.8277777777777777, 0.735], [0.682, 0.944, 0.8277777777777777, 0.735], [0.682, 0.944, 0.8277777777777777, 0.735], [0.682, 0.944, 0.8277777777777777, 0.735], [0.682, 0.944, 0.8277777777777777, 0.735], [0.682, 0.944, 0.8277777777777777, 0.735], [0.682, 0.944, 0.8277777777777777, 0.735], [0.682, 0.944, 0.8277777777777777, 0.735], [0.817, 0.605, 0.7166388888888889, 0.479], [0.817, 0.605, 0.7166388888888889, 0.479], [0.817, 0.605, 0.7166388888888889, 0.479], [0.817, 0.605, 0.7166388888888889, 0.479], [0.817, 0.605, 0.7166388888888889, 0.479], [0.817, 0.605, 0.7166388888888889, 0.479], [0.817, 0.605, 0.7166388888888889, 0.479], [0.817, 0.605, 0.7166388888888889, 0.479], [0.817, 0.605, 0.7166388888888889, 0.479], [0.817, 0.605, 0.7166388888888889, 0.479], [0.785, 0.831, 0.7003111111111111, 0.673], [0.785, 0.831, 0.7003111111111111, 0.673], [0.785, 0.831, 0.7003111111111111, 0.673], [0.785, 0.831, 0.7003111111111111, 0.673], [0.785, 0.831, 0.7003111111111111, 0.673], [0.785, 0.831, 0.7003111111111111, 0.673], [0.785, 0.831, 0.7003111111111111, 0.673], [0.785, 0.831, 0.7003111111111111, 0.673], [0.785, 0.831, 0.7003111111111111, 0.673], [0.785, 0.831, 0.7003111111111111, 0.673], [0.425, 0.984, 0.7103333333333334, 0.419], [0.425, 0.984, 0.7103333333333334, 0.419], [0.425, 0.984, 0.7103333333333334, 0.419], [0.425, 0.984, 0.7103333333333334, 0.419], [0.425, 0.984, 0.7103333333333334, 0.419], [0.425, 0.984, 0.7103333333333334, 0.419], [0.425, 0.984, 0.7103333333333334, 0.419], [0.425, 0.984, 0.7103333333333334, 0.419], [0.425, 0.984, 0.7103333333333334, 0.419], [0.425, 0.984, 0.7103333333333334, 0.419], [0.702, 0.92, 0.81115, 0.673], [0.702, 0.92, 0.81115, 0.673], [0.702, 0.92, 0.81115, 0.673], [0.702, 0.92, 0.81115, 0.673], [0.702, 0.92, 0.81115, 0.673], [0.702, 0.92, 0.81115, 0.673], [0.702, 0.92, 0.81115, 0.673], [0.702, 0.92, 0.81115, 0.673], [0.702, 0.92, 0.81115, 0.673], [0.702, 0.92, 0.81115, 0.673], [0.865, 0.978, 0.7222666666666667, 0.936], [0.865, 0.978, 0.7222666666666667, 0.936], [0.865, 0.978, 0.7222666666666667, 0.936], [0.865, 0.978, 0.7222666666666667, 0.936], [0.865, 0.978, 0.7222666666666667, 0.936], [0.865, 0.978, 0.7222666666666667, 0.936], [0.865, 0.978, 0.7222666666666667, 0.936], [0.865, 0.978, 0.7222666666666667, 0.936], [0.865, 0.978, 0.7222666666666667, 0.936], [0.865, 0.978, 0.7222666666666667, 0.936], [0.776, 0.667, 0.7223277777777778, 0.618], [0.776, 0.667, 0.7223277777777778, 0.618], [0.776, 0.667, 0.7223277777777778, 0.618], [0.776, 0.667, 0.7223277777777778, 0.618], [0.776, 0.667, 0.7223277777777778, 0.618], [0.776, 0.667, 0.7223277777777778, 0.618], [0.776, 0.667, 0.7223277777777778, 0.618], [0.776, 0.667, 0.7223277777777778, 0.618], [0.776, 0.667, 0.7223277777777778, 0.618], [0.776, 0.667, 0.7223277777777778, 0.618], [0.629, 0.824, 0.7111666666666666, 0.852], [0.629, 0.824, 0.7111666666666666, 0.852], [0.629, 0.824, 0.7111666666666666, 0.852], [0.629, 0.824, 0.7111666666666666, 0.852], [0.629, 0.824, 0.7111666666666666, 0.852], [0.629, 0.824, 0.7111666666666666, 0.852], [0.629, 0.824, 0.7111666666666666, 0.852], [0.629, 0.824, 0.7111666666666666, 0.852], [0.629, 0.824, 0.7111666666666666, 0.852], [0.629, 0.824, 0.7111666666666666, 0.852], [0.649, 0.931, 0.8535833333333334, 0.744], [0.649, 0.931, 0.8535833333333334, 0.744], [0.649, 0.931, 0.8535833333333334, 0.744], [0.649, 0.931, 0.8535833333333334, 0.744], [0.649, 0.931, 0.8535833333333334, 0.744], [0.649, 0.931, 0.8535833333333334, 0.744], [0.649, 0.931, 0.8535833333333334, 0.744], [0.649, 0.931, 0.8535833333333334, 0.744], [0.649, 0.931, 0.8535833333333334, 0.744], [0.649, 0.931, 0.8535833333333334, 0.744], [0.597, 0.989, 0.7610722222222222, 0.622], [0.597, 0.989, 0.7610722222222222, 0.622], [0.597, 0.989, 0.7610722222222222, 0.622], [0.597, 0.989, 0.7610722222222222, 0.622], [0.597, 0.989, 0.7610722222222222, 0.622], [0.597, 0.989, 0.7610722222222222, 0.622], [0.597, 0.989, 0.7610722222222222, 0.622], [0.597, 0.989, 0.7610722222222222, 0.622], [0.597, 0.989, 0.7610722222222222, 0.622], [0.597, 0.989, 0.7610722222222222, 0.622], [0.691, 0.963, 0.822211111111111, 0.55], [0.691, 0.963, 0.822211111111111, 0.55], [0.691, 0.963, 0.822211111111111, 0.55], [0.691, 0.963, 0.822211111111111, 0.55], [0.691, 0.963, 0.822211111111111, 0.55], [0.691, 0.963, 0.822211111111111, 0.55], [0.691, 0.963, 0.822211111111111, 0.55], [0.691, 0.963, 0.822211111111111, 0.55], [0.691, 0.963, 0.822211111111111, 0.55], [0.691, 0.963, 0.822211111111111, 0.55], [0.63, 0.945, 0.8111166666666667, 0.825], [0.63, 0.945, 0.8111166666666667, 0.825], [0.63, 0.945, 0.8111166666666667, 0.825], [0.63, 0.945, 0.8111166666666667, 0.825], [0.63, 0.945, 0.8111166666666667, 0.825], [0.63, 0.945, 0.8111166666666667, 0.825], [0.63, 0.945, 0.8111166666666667, 0.825], [0.63, 0.945, 0.8111166666666667, 0.825], [0.63, 0.945, 0.8111166666666667, 0.825], [0.63, 0.945, 0.8111166666666667, 0.825], [0.793, 0.779, 0.7889, 0.687], [0.793, 0.779, 0.7889, 0.687], [0.793, 0.779, 0.7889, 0.687], [0.793, 0.779, 0.7889, 0.687], [0.793, 0.779, 0.7889, 0.687], [0.793, 0.779, 0.7889, 0.687], [0.793, 0.779, 0.7889, 0.687], [0.793, 0.779, 0.7889, 0.687], [0.793, 0.779, 0.7889, 0.687], [0.793, 0.779, 0.7889, 0.687], [0.548, 0.99, 0.8054833333333333, 0.669], [0.548, 0.99, 0.8054833333333333, 0.669], [0.548, 0.99, 0.8054833333333333, 0.669], [0.548, 0.99, 0.8054833333333333, 0.669], [0.548, 0.99, 0.8054833333333333, 0.669], [0.548, 0.99, 0.8054833333333333, 0.669], [0.548, 0.99, 0.8054833333333333, 0.669], [0.548, 0.99, 0.8054833333333333, 0.669], [0.548, 0.99, 0.8054833333333333, 0.669], [0.548, 0.99, 0.8054833333333333, 0.669], [0.608, 0.995, 0.8052833333333334, 0.96], [0.608, 0.995, 0.8052833333333334, 0.96], [0.608, 0.995, 0.8052833333333334, 0.96], [0.608, 0.995, 0.8052833333333334, 0.96], [0.608, 0.995, 0.8052833333333334, 0.96], [0.608, 0.995, 0.8052833333333334, 0.96], [0.608, 0.995, 0.8052833333333334, 0.96], [0.608, 0.995, 0.8052833333333334, 0.96], [0.608, 0.995, 0.8052833333333334, 0.96], [0.608, 0.995, 0.8052833333333334, 0.96], [0.628, 0.938, 0.8332833333333334, 0.738], [0.628, 0.938, 0.8332833333333334, 0.738], [0.628, 0.938, 0.8332833333333334, 0.738], [0.628, 0.938, 0.8332833333333334, 0.738], [0.628, 0.938, 0.8332833333333334, 0.738], [0.628, 0.938, 0.8332833333333334, 0.738], [0.628, 0.938, 0.8332833333333334, 0.738], [0.628, 0.938, 0.8332833333333334, 0.738], [0.628, 0.938, 0.8332833333333334, 0.738], [0.628, 0.938, 0.8332833333333334, 0.738], [0.698, 0.981, 0.9666055555555556, 0.524], [0.698, 0.981, 0.9666055555555556, 0.524], [0.698, 0.981, 0.9666055555555556, 0.524], [0.698, 0.981, 0.9666055555555556, 0.524], [0.698, 0.981, 0.9666055555555556, 0.524], [0.698, 0.981, 0.9666055555555556, 0.524], [0.698, 0.981, 0.9666055555555556, 0.524], [0.698, 0.981, 0.9666055555555556, 0.524], [0.698, 0.981, 0.9666055555555556, 0.524], [0.698, 0.981, 0.9666055555555556, 0.524], [0.866, 0.925, 0.7666944444444445, 0.547], [0.866, 0.925, 0.7666944444444445, 0.547], [0.866, 0.925, 0.7666944444444445, 0.547], [0.866, 0.925, 0.7666944444444445, 0.547], [0.866, 0.925, 0.7666944444444445, 0.547], [0.866, 0.925, 0.7666944444444445, 0.547], [0.866, 0.925, 0.7666944444444445, 0.547], [0.866, 0.925, 0.7666944444444445, 0.547], [0.866, 0.925, 0.7666944444444445, 0.547], [0.866, 0.925, 0.7666944444444445, 0.547], [0.574, 0.873, 0.6838444444444445, 0.111], [0.574, 0.873, 0.6838444444444445, 0.111], [0.574, 0.873, 0.6838444444444445, 0.111], [0.574, 0.873, 0.6838444444444445, 0.111], [0.574, 0.873, 0.6838444444444445, 0.111], [0.574, 0.873, 0.6838444444444445, 0.111], [0.574, 0.873, 0.6838444444444445, 0.111], [0.574, 0.873, 0.6838444444444445, 0.111], [0.574, 0.873, 0.6838444444444445, 0.111], [0.574, 0.873, 0.6838444444444445, 0.111]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([270, 665])\n",
      "torch.Size([270, 4])\n",
      "tensor(0.7501)\n",
      "Epoch [100/5000], Training Loss: 0.0242, Validation Loss: 0.0244\n",
      "Epoch [200/5000], Training Loss: 0.0222, Validation Loss: 0.0228\n",
      "Epoch [300/5000], Training Loss: 0.0211, Validation Loss: 0.0217\n",
      "Epoch [400/5000], Training Loss: 0.0202, Validation Loss: 0.0209\n",
      "Epoch [500/5000], Training Loss: 0.0201, Validation Loss: 0.0202\n",
      "Epoch [600/5000], Training Loss: 0.0190, Validation Loss: 0.0197\n",
      "Epoch [700/5000], Training Loss: 0.0182, Validation Loss: 0.0192\n",
      "Epoch [800/5000], Training Loss: 0.0178, Validation Loss: 0.0189\n",
      "Epoch [900/5000], Training Loss: 0.0171, Validation Loss: 0.0186\n",
      "Epoch [1000/5000], Training Loss: 0.0172, Validation Loss: 0.0184\n",
      "Epoch [1100/5000], Training Loss: 0.0166, Validation Loss: 0.0183\n",
      "Epoch [1200/5000], Training Loss: 0.0163, Validation Loss: 0.0182\n",
      "tensor([[0.6895, 0.8864, 0.7824, 0.6288],\n",
      "        [0.7098, 0.8880, 0.7607, 0.6882],\n",
      "        [0.5851, 0.6987, 0.6414, 0.5405],\n",
      "        [0.6104, 0.7450, 0.6676, 0.5579],\n",
      "        [0.6910, 0.9401, 0.8257, 0.7208],\n",
      "        [0.6519, 0.8160, 0.6946, 0.6369],\n",
      "        [0.7146, 0.9169, 0.7857, 0.6372],\n",
      "        [0.6975, 0.9306, 0.8284, 0.6828],\n",
      "        [0.6869, 0.8703, 0.7869, 0.7354],\n",
      "        [0.6741, 0.8997, 0.7759, 0.6502],\n",
      "        [0.7056, 0.9101, 0.7836, 0.6665],\n",
      "        [0.7135, 0.8651, 0.7337, 0.6158],\n",
      "        [0.6529, 0.8948, 0.7880, 0.6428],\n",
      "        [0.6035, 0.7669, 0.6787, 0.6092],\n",
      "        [0.7123, 0.9234, 0.8100, 0.6546],\n",
      "        [0.6783, 0.8638, 0.7822, 0.6504]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7470, 0.8210, 0.8057, 0.7870],\n",
      "        [0.6290, 0.8240, 0.7112, 0.8520],\n",
      "        [0.5750, 0.9220, 0.8169, 0.9780],\n",
      "        [0.5750, 0.9220, 0.8169, 0.9780],\n",
      "        [0.6980, 0.9810, 0.9666, 0.5240],\n",
      "        [0.6290, 0.8240, 0.7112, 0.8520],\n",
      "        [0.6820, 0.9440, 0.8278, 0.7350],\n",
      "        [0.8650, 0.9780, 0.7223, 0.9360],\n",
      "        [0.6490, 0.9310, 0.8536, 0.7440],\n",
      "        [0.5480, 0.9900, 0.8055, 0.6690],\n",
      "        [0.6490, 0.9310, 0.8536, 0.7440],\n",
      "        [0.8650, 0.9780, 0.7223, 0.9360],\n",
      "        [0.6280, 0.9380, 0.8333, 0.7380],\n",
      "        [0.8660, 0.9250, 0.7667, 0.5470],\n",
      "        [0.6670, 0.9400, 0.7944, 0.8750],\n",
      "        [0.6080, 0.9950, 0.8053, 0.9600]])\n",
      "tensor([[0.6799, 0.9305, 0.7891, 0.6782],\n",
      "        [0.6744, 0.8550, 0.7727, 0.6325],\n",
      "        [0.6561, 0.8430, 0.7681, 0.6189],\n",
      "        [0.6108, 0.7321, 0.6684, 0.5918],\n",
      "        [0.6971, 0.8345, 0.7348, 0.5067],\n",
      "        [0.6910, 0.8144, 0.7259, 0.4514],\n",
      "        [0.6769, 0.8783, 0.7798, 0.6276],\n",
      "        [0.6993, 0.8965, 0.7848, 0.5365],\n",
      "        [0.6764, 0.8426, 0.7144, 0.6131],\n",
      "        [0.7291, 0.8588, 0.7963, 0.5780],\n",
      "        [0.6585, 0.9283, 0.7807, 0.7289],\n",
      "        [0.7007, 0.8882, 0.7809, 0.7121],\n",
      "        [0.6494, 0.8103, 0.7483, 0.5757],\n",
      "        [0.6777, 0.8880, 0.7777, 0.6700],\n",
      "        [0.7203, 0.9487, 0.8187, 0.6792],\n",
      "        [0.7018, 0.9260, 0.8098, 0.7309]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8650, 0.9780, 0.7223, 0.9360],\n",
      "        [0.6280, 0.9380, 0.8333, 0.7380],\n",
      "        [0.6820, 0.9440, 0.8278, 0.7350],\n",
      "        [0.6490, 0.9310, 0.8536, 0.7440],\n",
      "        [0.8170, 0.6050, 0.7166, 0.4790],\n",
      "        [0.7050, 0.6720, 0.6889, 0.2840],\n",
      "        [0.5480, 0.9900, 0.8055, 0.6690],\n",
      "        [0.5740, 0.8730, 0.6838, 0.1110],\n",
      "        [0.6670, 0.9400, 0.7944, 0.8750],\n",
      "        [0.7930, 0.7790, 0.7889, 0.6870],\n",
      "        [0.5970, 0.9890, 0.7611, 0.6220],\n",
      "        [0.7720, 0.9730, 0.7778, 0.8360],\n",
      "        [0.7930, 0.7790, 0.7889, 0.6870],\n",
      "        [0.6300, 0.9450, 0.8111, 0.8250],\n",
      "        [0.6910, 0.9630, 0.8222, 0.5500],\n",
      "        [0.6820, 0.9440, 0.8278, 0.7350]])\n",
      "tensor([[0.6508, 0.9005, 0.7964, 0.7081],\n",
      "        [0.6104, 0.7602, 0.6741, 0.5382],\n",
      "        [0.6921, 0.8867, 0.7384, 0.6137],\n",
      "        [0.6596, 0.8881, 0.7624, 0.6005],\n",
      "        [0.6477, 0.8332, 0.7412, 0.6476],\n",
      "        [0.7864, 0.8887, 0.7953, 0.5598],\n",
      "        [0.6719, 0.8618, 0.7282, 0.6265],\n",
      "        [0.6999, 0.9370, 0.8115, 0.7320],\n",
      "        [0.6788, 0.7797, 0.7397, 0.5622],\n",
      "        [0.7132, 0.9258, 0.8077, 0.7818],\n",
      "        [0.6699, 0.9125, 0.7826, 0.7249],\n",
      "        [0.7038, 0.8972, 0.7232, 0.5944],\n",
      "        [0.6988, 0.9238, 0.8221, 0.6146],\n",
      "        [0.6489, 0.9166, 0.7298, 0.6592],\n",
      "        [0.6896, 0.9197, 0.7969, 0.7083],\n",
      "        [0.7018, 0.9073, 0.8079, 0.6518]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8660, 0.9250, 0.7667, 0.5470],\n",
      "        [0.6990, 0.8840, 0.7115, 0.2870],\n",
      "        [0.7020, 0.9200, 0.8112, 0.6730],\n",
      "        [0.6490, 0.9310, 0.8536, 0.7440],\n",
      "        [0.5750, 0.9220, 0.8169, 0.9780],\n",
      "        [0.6990, 0.8840, 0.7115, 0.2870],\n",
      "        [0.6300, 0.9450, 0.8111, 0.8250],\n",
      "        [0.6980, 0.9810, 0.9666, 0.5240],\n",
      "        [0.5970, 0.9890, 0.7611, 0.6220],\n",
      "        [0.7720, 0.9730, 0.7778, 0.8360],\n",
      "        [0.7020, 0.9200, 0.8112, 0.6730],\n",
      "        [0.8660, 0.9250, 0.7667, 0.5470],\n",
      "        [0.8170, 0.6050, 0.7166, 0.4790],\n",
      "        [0.4250, 0.9840, 0.7103, 0.4190],\n",
      "        [0.7020, 0.9200, 0.8112, 0.6730],\n",
      "        [0.6980, 0.9810, 0.9666, 0.5240]])\n",
      "tensor([[0.6817, 0.9136, 0.8002, 0.5712],\n",
      "        [0.6240, 0.7766, 0.7034, 0.6108],\n",
      "        [0.6895, 0.9121, 0.7357, 0.5597],\n",
      "        [0.6604, 0.7886, 0.7223, 0.5914],\n",
      "        [0.6769, 0.9181, 0.8092, 0.6640],\n",
      "        [0.6946, 0.8630, 0.7209, 0.5414],\n",
      "        [0.6318, 0.8154, 0.7063, 0.6211],\n",
      "        [0.6782, 0.8987, 0.8008, 0.7806],\n",
      "        [0.6571, 0.8804, 0.7572, 0.5963],\n",
      "        [0.6956, 0.9043, 0.8026, 0.6856],\n",
      "        [0.6247, 0.8995, 0.7439, 0.6280],\n",
      "        [0.6808, 0.8629, 0.7554, 0.6671],\n",
      "        [0.6122, 0.7329, 0.6853, 0.5017],\n",
      "        [0.7294, 0.8851, 0.7501, 0.6254],\n",
      "        [0.6677, 0.8529, 0.7017, 0.5867],\n",
      "        [0.6432, 0.9374, 0.7993, 0.6712]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5740, 0.8730, 0.6838, 0.1110],\n",
      "        [0.5970, 0.9890, 0.7611, 0.6220],\n",
      "        [0.8170, 0.6050, 0.7166, 0.4790],\n",
      "        [0.8170, 0.6050, 0.7166, 0.4790],\n",
      "        [0.6910, 0.9630, 0.8222, 0.5500],\n",
      "        [0.5740, 0.8730, 0.6838, 0.1110],\n",
      "        [0.6300, 0.9450, 0.8111, 0.8250],\n",
      "        [0.8660, 0.9250, 0.7667, 0.5470],\n",
      "        [0.5740, 0.8730, 0.6838, 0.1110],\n",
      "        [0.7470, 0.8210, 0.8057, 0.7870],\n",
      "        [0.4250, 0.9840, 0.7103, 0.4190],\n",
      "        [0.4250, 0.9840, 0.7103, 0.4190],\n",
      "        [0.7050, 0.6720, 0.6889, 0.2840],\n",
      "        [0.6580, 0.6620, 0.6834, 0.2540],\n",
      "        [0.7270, 0.9260, 0.7501, 0.4170],\n",
      "        [0.7470, 0.8210, 0.8057, 0.7870]])\n",
      "tensor([[0.6665, 0.8502, 0.7466, 0.5500],\n",
      "        [0.6860, 0.8679, 0.7312, 0.6566],\n",
      "        [0.6632, 0.8702, 0.7845, 0.6269],\n",
      "        [0.6271, 0.8950, 0.7470, 0.6696],\n",
      "        [0.6320, 0.7821, 0.6910, 0.5631],\n",
      "        [0.7405, 0.8858, 0.7760, 0.6084],\n",
      "        [0.6755, 0.9077, 0.7843, 0.6268],\n",
      "        [0.6645, 0.9145, 0.7689, 0.6138],\n",
      "        [0.7142, 0.8664, 0.7539, 0.5287],\n",
      "        [0.6991, 0.8417, 0.7232, 0.4625],\n",
      "        [0.6519, 0.8574, 0.7313, 0.6567],\n",
      "        [0.6897, 0.7937, 0.7388, 0.5121],\n",
      "        [0.7060, 0.8690, 0.7930, 0.6672],\n",
      "        [0.6286, 0.7889, 0.6936, 0.6159],\n",
      "        [0.6993, 0.8113, 0.7463, 0.5780],\n",
      "        [0.6598, 0.9215, 0.8098, 0.6716]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5740, 0.8730, 0.6838, 0.1110],\n",
      "        [0.8660, 0.9250, 0.7667, 0.5470],\n",
      "        [0.7760, 0.6670, 0.7223, 0.6180],\n",
      "        [0.6980, 0.9810, 0.9666, 0.5240],\n",
      "        [0.7270, 0.9260, 0.7501, 0.4170],\n",
      "        [0.7470, 0.8210, 0.8057, 0.7870],\n",
      "        [0.7470, 0.8210, 0.8057, 0.7870],\n",
      "        [0.7760, 0.6670, 0.7223, 0.6180],\n",
      "        [0.7270, 0.9260, 0.7501, 0.4170],\n",
      "        [0.7050, 0.6720, 0.6889, 0.2840],\n",
      "        [0.6300, 0.9450, 0.8111, 0.8250],\n",
      "        [0.6580, 0.6620, 0.6834, 0.2540],\n",
      "        [0.5480, 0.9900, 0.8055, 0.6690],\n",
      "        [0.6080, 0.9950, 0.8053, 0.9600],\n",
      "        [0.7270, 0.9260, 0.7501, 0.4170],\n",
      "        [0.4250, 0.9840, 0.7103, 0.4190]])\n",
      "tensor([[0.6981, 0.8456, 0.7877, 0.6165],\n",
      "        [0.6937, 0.9092, 0.8122, 0.6722],\n",
      "        [0.6017, 0.7634, 0.6676, 0.5952],\n",
      "        [0.7050, 0.8301, 0.7525, 0.5936],\n",
      "        [0.7002, 0.8355, 0.7270, 0.4960],\n",
      "        [0.6781, 0.9042, 0.7816, 0.6711],\n",
      "        [0.6821, 0.9092, 0.7850, 0.6222],\n",
      "        [0.6327, 0.8015, 0.7063, 0.6110],\n",
      "        [0.7022, 0.9361, 0.8051, 0.7045],\n",
      "        [0.7083, 0.9065, 0.8135, 0.6596],\n",
      "        [0.7743, 0.9589, 0.8751, 0.6540],\n",
      "        [0.6799, 0.8971, 0.7928, 0.7485],\n",
      "        [0.6887, 0.8804, 0.7639, 0.6629],\n",
      "        [0.6479, 0.8274, 0.7065, 0.5671],\n",
      "        [0.6821, 0.8991, 0.7887, 0.6763],\n",
      "        [0.6737, 0.9320, 0.8158, 0.7503]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7760, 0.6670, 0.7223, 0.6180],\n",
      "        [0.6290, 0.8240, 0.7112, 0.8520],\n",
      "        [0.5480, 0.9900, 0.8055, 0.6690],\n",
      "        [0.7930, 0.7790, 0.7889, 0.6870],\n",
      "        [0.8170, 0.6050, 0.7166, 0.4790],\n",
      "        [0.7020, 0.9200, 0.8112, 0.6730],\n",
      "        [0.7470, 0.8210, 0.8057, 0.7870],\n",
      "        [0.6670, 0.9400, 0.7944, 0.8750],\n",
      "        [0.6670, 0.9400, 0.7944, 0.8750],\n",
      "        [0.5970, 0.9890, 0.7611, 0.6220],\n",
      "        [0.6280, 0.9380, 0.8333, 0.7380],\n",
      "        [0.7930, 0.7790, 0.7889, 0.6870],\n",
      "        [0.5970, 0.9890, 0.7611, 0.6220],\n",
      "        [0.6990, 0.8840, 0.7115, 0.2870],\n",
      "        [0.6980, 0.9810, 0.9666, 0.5240],\n",
      "        [0.6300, 0.9450, 0.8111, 0.8250]])\n",
      "tensor([[0.6689, 0.8889, 0.7882, 0.6351],\n",
      "        [0.6971, 0.8837, 0.7638, 0.5156],\n",
      "        [0.6768, 0.9052, 0.7943, 0.6051],\n",
      "        [0.7605, 0.8961, 0.7610, 0.6373],\n",
      "        [0.6977, 0.8598, 0.7953, 0.6575],\n",
      "        [0.6462, 0.9160, 0.7694, 0.6599],\n",
      "        [0.6715, 0.9347, 0.8184, 0.7318],\n",
      "        [0.6705, 0.9303, 0.7996, 0.7328],\n",
      "        [0.6631, 0.9444, 0.7930, 0.6816],\n",
      "        [0.6582, 0.9231, 0.7996, 0.6465],\n",
      "        [0.7312, 0.9032, 0.7792, 0.5899],\n",
      "        [0.6947, 0.8924, 0.7863, 0.7049],\n",
      "        [0.6131, 0.7762, 0.6958, 0.5525],\n",
      "        [0.6341, 0.9209, 0.7863, 0.6705],\n",
      "        [0.6733, 0.9414, 0.8163, 0.7311],\n",
      "        [0.6870, 0.9339, 0.8081, 0.7056]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7470, 0.8210, 0.8057, 0.7870],\n",
      "        [0.8170, 0.6050, 0.7166, 0.4790],\n",
      "        [0.5480, 0.9900, 0.8055, 0.6690],\n",
      "        [0.7850, 0.8310, 0.7003, 0.6730],\n",
      "        [0.7760, 0.6670, 0.7223, 0.6180],\n",
      "        [0.7470, 0.8210, 0.8057, 0.7870],\n",
      "        [0.5480, 0.9900, 0.8055, 0.6690],\n",
      "        [0.6080, 0.9950, 0.8053, 0.9600],\n",
      "        [0.6290, 0.8240, 0.7112, 0.8520],\n",
      "        [0.8660, 0.9250, 0.7667, 0.5470],\n",
      "        [0.8660, 0.9250, 0.7667, 0.5470],\n",
      "        [0.8660, 0.9250, 0.7667, 0.5470],\n",
      "        [0.6580, 0.6620, 0.6834, 0.2540],\n",
      "        [0.5970, 0.9890, 0.7611, 0.6220],\n",
      "        [0.6670, 0.9400, 0.7944, 0.8750],\n",
      "        [0.6820, 0.9440, 0.8278, 0.7350]])\n",
      "tensor([[0.7368, 0.8964, 0.7838, 0.6806],\n",
      "        [0.6178, 0.8352, 0.7156, 0.6280],\n",
      "        [0.6925, 0.8703, 0.7389, 0.5161],\n",
      "        [0.6924, 0.8559, 0.7610, 0.6541],\n",
      "        [0.6597, 0.7884, 0.7029, 0.5531],\n",
      "        [0.6007, 0.7267, 0.6574, 0.5449],\n",
      "        [0.6370, 0.8846, 0.7572, 0.7084],\n",
      "        [0.6210, 0.7334, 0.6859, 0.5321],\n",
      "        [0.6911, 0.9072, 0.7851, 0.5942],\n",
      "        [0.6829, 0.9227, 0.8057, 0.7028],\n",
      "        [0.6259, 0.7755, 0.7167, 0.6064],\n",
      "        [0.7312, 0.9089, 0.8189, 0.6773],\n",
      "        [0.6768, 0.8382, 0.6973, 0.6084],\n",
      "        [0.6722, 0.9193, 0.8060, 0.6189],\n",
      "        [0.6673, 0.8927, 0.7895, 0.6522],\n",
      "        [0.6400, 0.7897, 0.7188, 0.6072]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6080, 0.9950, 0.8053, 0.9600],\n",
      "        [0.6080, 0.9950, 0.8053, 0.9600],\n",
      "        [0.6990, 0.8840, 0.7115, 0.2870],\n",
      "        [0.5740, 0.8730, 0.6838, 0.1110],\n",
      "        [0.6580, 0.6620, 0.6834, 0.2540],\n",
      "        [0.7050, 0.6720, 0.6889, 0.2840],\n",
      "        [0.6280, 0.9380, 0.8333, 0.7380],\n",
      "        [0.7270, 0.9260, 0.7501, 0.4170],\n",
      "        [0.7470, 0.8210, 0.8057, 0.7870],\n",
      "        [0.5970, 0.9890, 0.7611, 0.6220],\n",
      "        [0.7850, 0.8310, 0.7003, 0.6730],\n",
      "        [0.7850, 0.8310, 0.7003, 0.6730],\n",
      "        [0.5750, 0.9220, 0.8169, 0.9780],\n",
      "        [0.7850, 0.8310, 0.7003, 0.6730],\n",
      "        [0.6300, 0.9450, 0.8111, 0.8250],\n",
      "        [0.6280, 0.9380, 0.8333, 0.7380]])\n",
      "tensor([[0.6855, 0.8331, 0.7136, 0.4157],\n",
      "        [0.6345, 0.7999, 0.7376, 0.4629],\n",
      "        [0.9624, 1.0000, 0.9999, 0.9979],\n",
      "        [0.6138, 0.7541, 0.6735, 0.5545],\n",
      "        [0.6112, 0.7740, 0.6792, 0.5317],\n",
      "        [0.6898, 0.8966, 0.7932, 0.6338],\n",
      "        [0.6882, 0.8978, 0.7898, 0.6285],\n",
      "        [0.7727, 0.9038, 0.8165, 0.5891],\n",
      "        [0.7104, 0.9281, 0.8225, 0.7094],\n",
      "        [0.6534, 0.8780, 0.7526, 0.6550],\n",
      "        [0.7472, 0.9034, 0.8107, 0.6873],\n",
      "        [0.6542, 0.9415, 0.8168, 0.6879],\n",
      "        [0.7120, 0.9250, 0.8405, 0.8035],\n",
      "        [0.6414, 0.8911, 0.7235, 0.5052],\n",
      "        [0.6938, 0.9075, 0.8088, 0.7409],\n",
      "        [0.6926, 0.9433, 0.8125, 0.7698]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6990, 0.8840, 0.7115, 0.2870],\n",
      "        [0.7050, 0.6720, 0.6889, 0.2840],\n",
      "        [0.6280, 0.9380, 0.8333, 0.7380],\n",
      "        [0.6990, 0.8840, 0.7115, 0.2870],\n",
      "        [0.6990, 0.8840, 0.7115, 0.2870],\n",
      "        [0.6980, 0.9810, 0.9666, 0.5240],\n",
      "        [0.7720, 0.9730, 0.7778, 0.8360],\n",
      "        [0.6820, 0.9440, 0.8278, 0.7350],\n",
      "        [0.6910, 0.9630, 0.8222, 0.5500],\n",
      "        [0.5970, 0.9890, 0.7611, 0.6220],\n",
      "        [0.6670, 0.9400, 0.7944, 0.8750],\n",
      "        [0.6300, 0.9450, 0.8111, 0.8250],\n",
      "        [0.6290, 0.8240, 0.7112, 0.8520],\n",
      "        [0.8660, 0.9250, 0.7667, 0.5470],\n",
      "        [0.7930, 0.7790, 0.7889, 0.6870],\n",
      "        [0.6820, 0.9440, 0.8278, 0.7350]])\n",
      "tensor([[0.6738, 0.8722, 0.7303, 0.6004],\n",
      "        [0.6997, 0.8370, 0.7461, 0.5179],\n",
      "        [0.6559, 0.9169, 0.7903, 0.7648],\n",
      "        [0.6865, 0.9240, 0.8140, 0.7625],\n",
      "        [0.6988, 0.9291, 0.7932, 0.6461],\n",
      "        [0.7022, 0.8635, 0.7379, 0.5922],\n",
      "        [0.6600, 0.9129, 0.7831, 0.6937],\n",
      "        [0.7013, 0.8129, 0.7734, 0.5858],\n",
      "        [0.6390, 0.8766, 0.6804, 0.6065],\n",
      "        [0.7318, 0.8457, 0.7696, 0.6424],\n",
      "        [0.6290, 0.8503, 0.7192, 0.6353],\n",
      "        [0.6788, 0.8528, 0.7187, 0.5036],\n",
      "        [0.6317, 0.9069, 0.8032, 0.7928],\n",
      "        [0.6753, 0.9221, 0.7891, 0.7047],\n",
      "        [0.6310, 0.7778, 0.6917, 0.5732],\n",
      "        [0.6735, 0.8693, 0.7000, 0.5625]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.5740, 0.8730, 0.6838, 0.1110],\n",
      "        [0.6580, 0.6620, 0.6834, 0.2540],\n",
      "        [0.6490, 0.9310, 0.8536, 0.7440],\n",
      "        [0.6490, 0.9310, 0.8536, 0.7440],\n",
      "        [0.7720, 0.9730, 0.7778, 0.8360],\n",
      "        [0.8650, 0.9780, 0.7223, 0.9360],\n",
      "        [0.4250, 0.9840, 0.7103, 0.4190],\n",
      "        [0.7760, 0.6670, 0.7223, 0.6180],\n",
      "        [0.6290, 0.8240, 0.7112, 0.8520],\n",
      "        [0.7760, 0.6670, 0.7223, 0.6180],\n",
      "        [0.7760, 0.6670, 0.7223, 0.6180],\n",
      "        [0.7270, 0.9260, 0.7501, 0.4170],\n",
      "        [0.6080, 0.9950, 0.8053, 0.9600],\n",
      "        [0.6280, 0.9380, 0.8333, 0.7380],\n",
      "        [0.5750, 0.9220, 0.8169, 0.9780],\n",
      "        [0.6490, 0.9310, 0.8536, 0.7440]])\n",
      "tensor([[0.6855, 0.8266, 0.7027, 0.5141],\n",
      "        [0.6842, 0.9197, 0.7644, 0.6737],\n",
      "        [0.6620, 0.8839, 0.7686, 0.6263],\n",
      "        [0.7246, 0.8799, 0.7978, 0.7268],\n",
      "        [0.6713, 0.9069, 0.7499, 0.6319],\n",
      "        [0.6474, 0.9185, 0.7535, 0.5567],\n",
      "        [0.6957, 0.9357, 0.8216, 0.7508],\n",
      "        [0.7684, 0.8626, 0.7942, 0.6265],\n",
      "        [0.7425, 0.9068, 0.7668, 0.6575],\n",
      "        [0.6761, 0.8534, 0.7632, 0.7078],\n",
      "        [0.6856, 0.9001, 0.7825, 0.6666],\n",
      "        [0.6823, 0.8200, 0.7024, 0.4858],\n",
      "        [0.6626, 0.8565, 0.7663, 0.6901],\n",
      "        [0.6291, 0.7924, 0.6907, 0.5944],\n",
      "        [0.6969, 0.8648, 0.7603, 0.6267],\n",
      "        [0.6925, 0.9290, 0.8188, 0.6906]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7020, 0.9200, 0.8112, 0.6730],\n",
      "        [0.7720, 0.9730, 0.7778, 0.8360],\n",
      "        [0.5970, 0.9890, 0.7611, 0.6220],\n",
      "        [0.7720, 0.9730, 0.7778, 0.8360],\n",
      "        [0.7020, 0.9200, 0.8112, 0.6730],\n",
      "        [0.4250, 0.9840, 0.7103, 0.4190],\n",
      "        [0.7760, 0.6670, 0.7223, 0.6180],\n",
      "        [0.7720, 0.9730, 0.7778, 0.8360],\n",
      "        [0.6820, 0.9440, 0.8278, 0.7350],\n",
      "        [0.8650, 0.9780, 0.7223, 0.9360],\n",
      "        [0.7930, 0.7790, 0.7889, 0.6870],\n",
      "        [0.5740, 0.8730, 0.6838, 0.1110],\n",
      "        [0.6990, 0.8840, 0.7115, 0.2870],\n",
      "        [0.8660, 0.9250, 0.7667, 0.5470],\n",
      "        [0.6910, 0.9630, 0.8222, 0.5500],\n",
      "        [0.8650, 0.9780, 0.7223, 0.9360]])\n",
      "tensor([[0.6881, 0.8789, 0.7747, 0.7029],\n",
      "        [0.6880, 0.8806, 0.7976, 0.6409],\n",
      "        [0.6566, 0.8367, 0.7187, 0.5988],\n",
      "        [0.6091, 0.7414, 0.6620, 0.5331],\n",
      "        [0.7184, 0.9340, 0.8060, 0.7338],\n",
      "        [0.6321, 0.7883, 0.7117, 0.6133],\n",
      "        [0.6487, 0.8983, 0.7539, 0.6326],\n",
      "        [0.6842, 0.8943, 0.7955, 0.6516],\n",
      "        [0.6168, 0.7505, 0.6837, 0.5450],\n",
      "        [0.6298, 0.8683, 0.7479, 0.6393],\n",
      "        [0.6906, 0.8239, 0.7180, 0.5896],\n",
      "        [0.7072, 0.9345, 0.8087, 0.7336],\n",
      "        [0.6500, 0.9046, 0.8166, 0.7720],\n",
      "        [0.7086, 0.8643, 0.7861, 0.6096],\n",
      "        [0.6952, 0.9327, 0.8089, 0.7634],\n",
      "        [0.7276, 0.8678, 0.7869, 0.6224]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.8650, 0.9780, 0.7223, 0.9360],\n",
      "        [0.7850, 0.8310, 0.7003, 0.6730],\n",
      "        [0.6910, 0.9630, 0.8222, 0.5500],\n",
      "        [0.6580, 0.6620, 0.6834, 0.2540],\n",
      "        [0.5480, 0.9900, 0.8055, 0.6690],\n",
      "        [0.5480, 0.9900, 0.8055, 0.6690],\n",
      "        [0.4250, 0.9840, 0.7103, 0.4190],\n",
      "        [0.5480, 0.9900, 0.8055, 0.6690],\n",
      "        [0.7050, 0.6720, 0.6889, 0.2840],\n",
      "        [0.6300, 0.9450, 0.8111, 0.8250],\n",
      "        [0.5750, 0.9220, 0.8169, 0.9780],\n",
      "        [0.6300, 0.9450, 0.8111, 0.8250],\n",
      "        [0.6080, 0.9950, 0.8053, 0.9600],\n",
      "        [0.7930, 0.7790, 0.7889, 0.6870],\n",
      "        [0.7930, 0.7790, 0.7889, 0.6870],\n",
      "        [0.7720, 0.9730, 0.7778, 0.8360]])\n",
      "tensor([[0.6602, 0.8936, 0.8059, 0.7839],\n",
      "        [0.6804, 0.8821, 0.7968, 0.6888],\n",
      "        [0.6454, 0.9057, 0.7431, 0.5846],\n",
      "        [0.6051, 0.7414, 0.6660, 0.5344],\n",
      "        [0.7131, 0.9223, 0.8008, 0.7589],\n",
      "        [0.6448, 0.8656, 0.7436, 0.7196],\n",
      "        [0.7292, 0.9103, 0.7722, 0.4732],\n",
      "        [0.7225, 0.8743, 0.7495, 0.6169],\n",
      "        [0.6999, 0.8839, 0.7722, 0.6019],\n",
      "        [0.6745, 0.9445, 0.8057, 0.6457],\n",
      "        [0.7026, 0.8832, 0.8149, 0.6560],\n",
      "        [0.6789, 0.9323, 0.7803, 0.6441],\n",
      "        [0.6931, 0.9180, 0.7265, 0.6419],\n",
      "        [0.6571, 0.9181, 0.7841, 0.6112],\n",
      "        [0.7380, 0.9216, 0.8655, 0.7411],\n",
      "        [0.7015, 0.9015, 0.8056, 0.7507]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.7930, 0.7790, 0.7889, 0.6870],\n",
      "        [0.8650, 0.9780, 0.7223, 0.9360],\n",
      "        [0.8170, 0.6050, 0.7166, 0.4790],\n",
      "        [0.6580, 0.6620, 0.6834, 0.2540],\n",
      "        [0.7020, 0.9200, 0.8112, 0.6730],\n",
      "        [0.5480, 0.9900, 0.8055, 0.6690],\n",
      "        [0.5740, 0.8730, 0.6838, 0.1110],\n",
      "        [0.7850, 0.8310, 0.7003, 0.6730],\n",
      "        [0.7930, 0.7790, 0.7889, 0.6870],\n",
      "        [0.5970, 0.9890, 0.7611, 0.6220],\n",
      "        [0.4250, 0.9840, 0.7103, 0.4190],\n",
      "        [0.6670, 0.9400, 0.7944, 0.8750],\n",
      "        [0.7850, 0.8310, 0.7003, 0.6730],\n",
      "        [0.6670, 0.9400, 0.7944, 0.8750],\n",
      "        [0.7470, 0.8210, 0.8057, 0.7870],\n",
      "        [0.6290, 0.8240, 0.7112, 0.8520]])\n",
      "tensor([[0.6615, 0.9044, 0.8052, 0.7721],\n",
      "        [0.6654, 0.9171, 0.7855, 0.5568],\n",
      "        [0.6167, 0.7235, 0.6771, 0.5292],\n",
      "        [0.7611, 0.9030, 0.7936, 0.7165],\n",
      "        [0.7099, 0.8797, 0.7831, 0.7193],\n",
      "        [0.7038, 0.8738, 0.8085, 0.6140],\n",
      "        [0.6609, 0.8903, 0.7858, 0.6324],\n",
      "        [0.6042, 0.7428, 0.6817, 0.5454]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0.6080, 0.9950, 0.8053, 0.9600],\n",
      "        [0.4250, 0.9840, 0.7103, 0.4190],\n",
      "        [0.7050, 0.6720, 0.6889, 0.2840],\n",
      "        [0.6080, 0.9950, 0.8053, 0.9600],\n",
      "        [0.6670, 0.9400, 0.7944, 0.8750],\n",
      "        [0.7020, 0.9200, 0.8112, 0.6730],\n",
      "        [0.6980, 0.9810, 0.9666, 0.5240],\n",
      "        [0.6580, 0.6620, 0.6834, 0.2540]])\n",
      "Epoch [1300/5000], Training Loss: 0.0160, Validation Loss: 0.0181\n",
      "Epoch [1400/5000], Training Loss: 0.0156, Validation Loss: 0.0180\n",
      "Epoch [1500/5000], Training Loss: 0.0154, Validation Loss: 0.0180\n",
      "Epoch [1600/5000], Training Loss: 0.0152, Validation Loss: 0.0180\n",
      "Epoch [1700/5000], Training Loss: 0.0152, Validation Loss: 0.0180\n",
      "Epoch [1800/5000], Training Loss: 0.0148, Validation Loss: 0.0180\n",
      "Epoch [1900/5000], Training Loss: 0.0148, Validation Loss: 0.0181\n",
      "Epoch [2000/5000], Training Loss: 0.0145, Validation Loss: 0.0181\n",
      "Epoch [2100/5000], Training Loss: 0.0140, Validation Loss: 0.0181\n",
      "Epoch [2200/5000], Training Loss: 0.0143, Validation Loss: 0.0182\n",
      "Epoch [2300/5000], Training Loss: 0.0138, Validation Loss: 0.0182\n",
      "Epoch [2400/5000], Training Loss: 0.0141, Validation Loss: 0.0183\n",
      "Epoch [2500/5000], Training Loss: 0.0140, Validation Loss: 0.0184\n",
      "Epoch [2600/5000], Training Loss: 0.0133, Validation Loss: 0.0184\n",
      "Epoch [2700/5000], Training Loss: 0.0132, Validation Loss: 0.0185\n",
      "Epoch [2800/5000], Training Loss: 0.0134, Validation Loss: 0.0186\n",
      "Epoch [2900/5000], Training Loss: 0.0131, Validation Loss: 0.0186\n",
      "Epoch [3000/5000], Training Loss: 0.0130, Validation Loss: 0.0187\n",
      "Epoch [3100/5000], Training Loss: 0.0127, Validation Loss: 0.0187\n",
      "Epoch [3200/5000], Training Loss: 0.0129, Validation Loss: 0.0188\n",
      "Epoch [3300/5000], Training Loss: 0.0126, Validation Loss: 0.0188\n",
      "Epoch [3400/5000], Training Loss: 0.0123, Validation Loss: 0.0189\n",
      "Epoch [3500/5000], Training Loss: 0.0123, Validation Loss: 0.0189\n",
      "Epoch [3600/5000], Training Loss: 0.0122, Validation Loss: 0.0190\n",
      "Epoch [3700/5000], Training Loss: 0.0121, Validation Loss: 0.0190\n",
      "Epoch [3800/5000], Training Loss: 0.0121, Validation Loss: 0.0191\n",
      "Epoch [3900/5000], Training Loss: 0.0123, Validation Loss: 0.0191\n",
      "Epoch [4000/5000], Training Loss: 0.0118, Validation Loss: 0.0192\n",
      "Epoch [4100/5000], Training Loss: 0.0115, Validation Loss: 0.0192\n",
      "Epoch [4200/5000], Training Loss: 0.0116, Validation Loss: 0.0193\n",
      "Epoch [4300/5000], Training Loss: 0.0117, Validation Loss: 0.0193\n",
      "Epoch [4400/5000], Training Loss: 0.0118, Validation Loss: 0.0194\n",
      "Epoch [4500/5000], Training Loss: 0.0113, Validation Loss: 0.0194\n",
      "Epoch [4600/5000], Training Loss: 0.0115, Validation Loss: 0.0194\n",
      "Epoch [4700/5000], Training Loss: 0.0113, Validation Loss: 0.0195\n",
      "Epoch [4800/5000], Training Loss: 0.0110, Validation Loss: 0.0195\n",
      "Epoch [4900/5000], Training Loss: 0.0110, Validation Loss: 0.0196\n",
      "Epoch [5000/5000], Training Loss: 0.0112, Validation Loss: 0.0196\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Define a simple regression model\n",
    "class SimpleRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(665, 32)\n",
    "        self.linear2 = nn.Linear(32, 4)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear2(self.linear(x)))\n",
    "\n",
    "tx = torch.tensor(x).squeeze()\n",
    "print(tx.shape)\n",
    "ty = torch.tensor(y)\n",
    "print(ty.shape)\n",
    "print(ty[1][2])\n",
    "\n",
    "# Create dataset and data loader\n",
    "dataset = TensorDataset(tx, ty)\n",
    "train_dataset, val_dataset = random_split(dataset, [.8,.2])\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function and optimizer\n",
    "model = SimpleRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.002)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5000\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        if epoch == 1200:\n",
    "            print(outputs)\n",
    "            print(targets)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item()\n",
    "    \n",
    "    # Compute average training loss for the epoch\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    # Validation step\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            running_val_loss += loss.item()\n",
    "    \n",
    "    # Compute average validation loss for the epoch\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'simple_regression_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnx in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (1.16.2)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (1.19.2)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from onnx) (1.24.3)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from onnx) (5.28.2)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from onnxruntime) (24.3.25)\n",
      "Requirement already satisfied: packaging in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from onnxruntime) (24.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from onnxruntime) (1.13.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from sympy->onnxruntime) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime) (3.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = tx[0] # Batch size, Number of features\n",
    "torch.onnx.export(model, dummy_input, 'simple_regression_model.onnx', input_names=['input'], output_names=['output'], dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def new_decoder(encoder_output: List, simple_regression_compiled : CompiledModel) -> List:\n",
    "    \"\"\"\n",
    "    Decoder inference per set of frames. This function concatenates the embedding layer\n",
    "    froms the encoder output, transpose the array to match with the decoder input size.\n",
    "    Calls the network previously configured for the decoder model (compiled_model_de), extracts\n",
    "    the logits and normalize those to get confidence values along specified axis.\n",
    "    Decodes top probabilities into corresponding label names\n",
    "\n",
    "    :param: encoder_output: embedding layer for 16 frames\n",
    "    :param: compiled_model_de: Decoder model network\n",
    "    :returns: decoded_labels: The k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence for the k most probable actions\n",
    "    \"\"\"\n",
    "    # Concatenate sample_duration frames in just one array\n",
    "    decoder_input = np.concatenate(encoder_output, axis=0)\n",
    "    # Organize input shape vector to the Decoder (shape: [1x16x512]]\n",
    "    decoder_input = decoder_input.squeeze()\n",
    "    #output_key_de = simple_regression_compiled.output(0)\n",
    "    # Get results on action-recognition-0001-decoder model\n",
    "    result_de = simple_regression_compiled([decoder_input])\n",
    "    # Normalize logits to get confidence values along specified axis\n",
    "    #probs = softmax(result_de - np.max(result_de))\n",
    "    # Decodes top probabilities into corresponding label names\n",
    "    #decoded_labels, decoded_top_probs = decode_output(probs, labels, top_k=3)\n",
    "    return result_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Decoder initialization\n",
    "sim_de, sim_out_de, simple_regression_compiled = model_init(\"simple_regression_model.onnx\", device.value)\n",
    "frames2decode = 1\n",
    "\n",
    "predictions = [0 for i in range(20)]\n",
    "\n",
    "def run_ai_dj(use_webcam):\n",
    "    global predictions\n",
    "    \n",
    "    source = \"1.mp4\"\n",
    "\n",
    "    skip_first_frames = 0\n",
    "    flip = True\n",
    "    size = height_en  # Endoder input size - From Cell 5_9\n",
    "    sample_duration = frames2decode  # Decoder input size - From Cell 5_7\n",
    "    # Select frames per second of your source.\n",
    "    size = height_en  # Endoder input size - From Cell 5_9\n",
    "    sample_duration = frames2decode  # Decoder input size - From Cell 5_7\n",
    "    # Select frames per second of your source.\n",
    "    fps = 30\n",
    "    player = None\n",
    "    try:\n",
    "        # Create a video player.\n",
    "        player = utils.VideoPlayer(0 if use_webcam else source, flip=flip, fps=fps, skip_first_frames=skip_first_frames)\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        processing_time = 0\n",
    "        encoder_output = []\n",
    "        decoded_labels = [0, 0, 0]\n",
    "        decoded_top_probs = [0, 0, 0]\n",
    "        counter = 0\n",
    "        # Create a text template to show inference results over video.\n",
    "        text_inference_template = \"Infer Time:{Time:.1f}ms,{fps:.1f}FPS\"\n",
    "        text_template = \"{label},{conf:.2f}%\"\n",
    "\n",
    "        while True:\n",
    "            counter = counter + 1\n",
    "\n",
    "            # Read a frame from the video stream.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "\n",
    "            scale = 1280 / max(frame.shape)\n",
    "\n",
    "            # Adaptative resize for visualization.\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "            # Select one frame every two for processing through the encoder.\n",
    "            # After 16 frames are processed, the decoder will find the action,\n",
    "            # and the label will be printed over the frames.\n",
    "\n",
    "            time.sleep(.1)\n",
    "\n",
    "            if counter % 60 == 0:\n",
    "                # Preprocess frame before Encoder.\n",
    "                (preprocessed, _) = preprocessing(frame, size)\n",
    "\n",
    "                # Measure processing time.\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Encoder Inference per frame\n",
    "                encoder_output.append(encoder(preprocessed, compiled_model_en))\n",
    "\n",
    "                # Decoder inference per set of frames\n",
    "                # Wait for sample duration to work with decoder model.\n",
    "                if len(encoder_output) == sample_duration:\n",
    "                    decoded_output= simple_regression_compiled(torch.tensor(encoder_output).squeeze())\n",
    "                    encoder_output = []\n",
    "                    print(decoded_output)\n",
    "                    predictions.append(decoded_output[0])\n",
    "                    predictions.remove(predictions[0])\n",
    "\n",
    "                # Inference has finished. Display the results.\n",
    "                stop_time = time.time()\n",
    "\n",
    "                # Calculate processing time.\n",
    "                processing_times.append(stop_time - start_time)\n",
    "\n",
    "                # Use processing times from last 200 frames.\n",
    "                if len(processing_times) > 200:\n",
    "                    processing_times.popleft()\n",
    "\n",
    "                # Mean processing time [ms]\n",
    "                processing_time = np.mean(processing_times) * 1000\n",
    "                fps = 1000 / processing_time\n",
    "                \n",
    "\n",
    "\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "        player.close()\n",
    "    # Any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ai_dj(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Flask\n",
      "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting Werkzeug>=3.0.0 (from Flask)\n",
      "  Downloading werkzeug-3.0.4-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from Flask) (3.1.4)\n",
      "Collecting itsdangerous>=2.1.2 (from Flask)\n",
      "  Downloading itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: click>=8.1.3 in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from Flask) (8.1.7)\n",
      "Collecting blinker>=1.6.2 (from Flask)\n",
      "  Downloading blinker-1.8.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from Flask) (8.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from click>=8.1.3->Flask) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from importlib-metadata>=3.6.0->Flask) (3.20.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\intelaipc\\anaconda3\\envs\\openmmlab\\lib\\site-packages (from Jinja2>=3.1.2->Flask) (2.1.3)\n",
      "Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
      "Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
      "Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Downloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
      "Installing collected packages: Werkzeug, itsdangerous, blinker, Flask\n",
      "Successfully installed Flask-3.0.3 Werkzeug-3.0.4 blinker-1.8.2 itsdangerous-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "#MODEL SERVING\n",
    "\n",
    "from flask import Flask, jsonify\n",
    "import random\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/get_floats', methods=['GET'])\n",
    "def get_floats():\n",
    "    # Generate four random floats\n",
    "    random_floats = [random.uniform(0, 1) for _ in range(4)]\n",
    "    # Return the floats in a JSON response\n",
    "    return jsonify(random_floats)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/10940214/151552326-642d6e49-f5a0-4fc1-bf14-ae3f457e1fec.gif",
   "tags": {
    "categories": [
     "Live Demos"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Image Classification"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
